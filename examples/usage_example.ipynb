{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0116dda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import textwrap\n",
    "from typing import cast\n",
    "\n",
    "\n",
    "\n",
    "import langextract as lx\n",
    "from IPython.display import HTML\n",
    "from langextract.data import AnnotatedDocument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bdb140b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the prompt and extraction rules\n",
    "prompt = textwrap.dedent(\"\"\"\\\n",
    "    Extract characters, emotions, and relationships in order of appearance.\n",
    "    Use exact text for extractions. Do not paraphrase or overlap entities.\n",
    "    Provide meaningful attributes for each entity to add context.\"\"\")\n",
    "\n",
    "# 2. Provide a high-quality example to guide the model\n",
    "examples = [\n",
    "    lx.data.ExampleData(\n",
    "        text=\"ROMEO. But soft! What light through yonder window breaks? It is the east, and Juliet is the sun.\",\n",
    "        extractions=[\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"character\",\n",
    "                extraction_text=\"ROMEO\",\n",
    "                attributes={\"emotional_state\": \"wonder\"},\n",
    "            ),\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"emotion\",\n",
    "                extraction_text=\"But soft!\",\n",
    "                attributes={\"feeling\": \"gentle awe\"},\n",
    "            ),\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"relationship\",\n",
    "                extraction_text=\"Juliet is the sun\",\n",
    "                attributes={\"type\": \"metaphor\"},\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2e790dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-09 22:55:55 [__init__.py:241] Automatically detected platform cuda.\n",
      "INFO 09-09 22:55:57 [utils.py:326] non-default args: {'model': 'microsoft/Phi-3-mini-4k-instruct', 'max_model_len': 1024, 'tensor_parallel_size': 2, 'gpu_memory_utilization': 0.5, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True}\n",
      "INFO 09-09 22:56:05 [__init__.py:711] Resolved architecture: Phi3ForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-09 22:56:05 [__init__.py:1750] Using max model len 1024\n",
      "INFO 09-09 22:56:07 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-09 22:56:07 [__init__.py:3565] Cudagraph is disabled under eager mode\n",
      "\u001b[1;36m(EngineCore_0 pid=93769)\u001b[0;0m INFO 09-09 22:56:08 [core.py:636] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_0 pid=93769)\u001b[0;0m INFO 09-09 22:56:08 [core.py:74] Initializing a V1 LLM engine (v0.10.1.1) with config: model='microsoft/Phi-3-mini-4k-instruct', speculative_config=None, tokenizer='microsoft/Phi-3-mini-4k-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=microsoft/Phi-3-mini-4k-instruct, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":0,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":0,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_0 pid=93769)\u001b[0;0m WARNING 09-09 22:56:08 [multiproc_worker_utils.py:273] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "\u001b[1;36m(EngineCore_0 pid=93769)\u001b[0;0m INFO 09-09 22:56:08 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 16777216, 10, 'psm_2968c836'), local_subscribe_addr='ipc:///tmp/6bc6d215-a31a-4f3c-81e9-3ee1aa9094f7', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(EngineCore_0 pid=93769)\u001b[0;0m \u001b[1;36m(VllmWorker TP0 pid=93791)\u001b[0;0m INFO 09-09 22:56:10 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_1eecc15f'), local_subscribe_addr='ipc:///tmp/8f72cfda-b84d-4047-b2e1-df66d33447b5', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(EngineCore_0 pid=93769)\u001b[0;0m \u001b[1;36m(VllmWorker TP1 pid=93793)\u001b[0;0m INFO 09-09 22:56:10 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_7c46cf88'), local_subscribe_addr='ipc:///tmp/ebd42f77-c56a-4cab-943d-1ca82919c70c', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(EngineCore_0 pid=93769)\u001b[0;0m \u001b[1;36m(VllmWorker TP0 pid=93791)\u001b[0;0m \u001b[1;36m(EngineCore_0 pid=93769)\u001b[0;0m INFO 09-09 22:56:16 [__init__.py:1418] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker TP1 pid=93793)\u001b[0;0m INFO 09-09 22:56:16 [__init__.py:1418] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(EngineCore_0 pid=93769)\u001b[0;0m \u001b[1;36m(EngineCore_0 pid=93769)\u001b[0;0m \u001b[1;36m(VllmWorker TP0 pid=93791)\u001b[0;0m \u001b[1;36m(VllmWorker TP1 pid=93793)\u001b[0;0m INFO 09-09 22:56:16 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 09-09 22:56:16 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(EngineCore_0 pid=93769)\u001b[0;0m \u001b[1;36m(VllmWorker TP0 pid=93791)\u001b[0;0m INFO 09-09 22:56:16 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_5c94a636'), local_subscribe_addr='ipc:///tmp/641a539a-18bd-4d7c-8887-9fbfd4a304a8', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(EngineCore_0 pid=93769)\u001b[0;0m \u001b[1;36m(EngineCore_0 pid=93769)\u001b[0;0m \u001b[1;36m(VllmWorker TP0 pid=93791)\u001b[0;0m \u001b[1;36m(VllmWorker TP1 pid=93793)\u001b[0;0m INFO 09-09 22:56:16 [parallel_state.py:1134] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-09 22:56:16 [parallel_state.py:1134] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(EngineCore_0 pid=93769)\u001b[0;0m \u001b[1;36m(VllmWorker TP1 pid=93793)\u001b[0;0m WARNING 09-09 22:56:16 [topk_topp_sampler.py:61] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_0 pid=93769)\u001b[0;0m \u001b[1;36m(VllmWorker TP0 pid=93791)\u001b[0;0m WARNING 09-09 22:56:16 [topk_topp_sampler.py:61] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_0 pid=93769)\u001b[0;0m \u001b[1;36m(VllmWorker TP1 pid=93793)\u001b[0;0m INFO 09-09 22:56:16 [gpu_model_runner.py:1953] Starting to load model microsoft/Phi-3-mini-4k-instruct...\n",
      "\u001b[1;36m(EngineCore_0 pid=93769)\u001b[0;0m \u001b[1;36m(VllmWorker TP0 pid=93791)\u001b[0;0m INFO 09-09 22:56:16 [gpu_model_runner.py:1953] Starting to load model microsoft/Phi-3-mini-4k-instruct...\n",
      "\u001b[1;36m(EngineCore_0 pid=93769)\u001b[0;0m \u001b[1;36m(VllmWorker TP1 pid=93793)\u001b[0;0m INFO 09-09 22:56:17 [gpu_model_runner.py:1985] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_0 pid=93769)\u001b[0;0m \u001b[1;36m(VllmWorker TP0 pid=93791)\u001b[0;0m INFO 09-09 22:56:17 [gpu_model_runner.py:1985] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_0 pid=93769)\u001b[0;0m \u001b[1;36m(VllmWorker TP1 pid=93793)\u001b[0;0m INFO 09-09 22:56:17 [cuda.py:328] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_0 pid=93769)\u001b[0;0m \u001b[1;36m(VllmWorker TP0 pid=93791)\u001b[0;0m INFO 09-09 22:56:17 [cuda.py:328] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_0 pid=93769)\u001b[0;0m \u001b[1;36m(VllmWorker TP0 pid=93791)\u001b[0;0m INFO 09-09 22:56:18 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(EngineCore_0 pid=93769)\u001b[0;0m \u001b[1;36m(VllmWorker TP1 pid=93793)\u001b[0;0m INFO 09-09 22:56:18 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(EngineCore_0 pid=93769)\u001b[0;0m \u001b[1;36m(VllmWorker TP0 pid=93791)\u001b[0;0m INFO 09-09 22:56:19 [weight_utils.py:312] Time spent downloading weights for microsoft/Phi-3-mini-4k-instruct: 0.846675 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f874d539a894d4f93841c50ae153c5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=93769)\u001b[0;0m \u001b[1;36m(VllmWorker TP1 pid=93793)\u001b[0;0m INFO 09-09 22:56:20 [weight_utils.py:312] Time spent downloading weights for microsoft/Phi-3-mini-4k-instruct: 0.646233 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=93769)\u001b[0;0m \u001b[1;36m(VllmWorker TP0 pid=93791)\u001b[0;0m INFO 09-09 22:56:20 [default_loader.py:262] Loading weights took 0.95 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=93769)\u001b[0;0m \u001b[1;36m(VllmWorker TP0 pid=93791)\u001b[0;0m INFO 09-09 22:56:20 [gpu_model_runner.py:2007] Model loading took 3.5911 GiB and 3.298806 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=93769)\u001b[0;0m \u001b[1;36m(VllmWorker TP1 pid=93793)\u001b[0;0m INFO 09-09 22:56:21 [default_loader.py:262] Loading weights took 0.89 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=93769)\u001b[0;0m \u001b[1;36m(VllmWorker TP1 pid=93793)\u001b[0;0m INFO 09-09 22:56:22 [gpu_model_runner.py:2007] Model loading took 3.5911 GiB and 4.400769 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=93769)\u001b[0;0m \u001b[1;36m(VllmWorker TP1 pid=93793)\u001b[0;0m INFO 09-09 22:56:25 [gpu_worker.py:276] Available KV cache memory: 7.79 GiB\n",
      "\u001b[1;36m(EngineCore_0 pid=93769)\u001b[0;0m \u001b[1;36m(VllmWorker TP0 pid=93791)\u001b[0;0m INFO 09-09 22:56:25 [gpu_worker.py:276] Available KV cache memory: 7.79 GiB\n",
      "\u001b[1;36m(EngineCore_0 pid=93769)\u001b[0;0m INFO 09-09 22:56:25 [kv_cache_utils.py:849] GPU KV cache size: 42,544 tokens\n",
      "\u001b[1;36m(EngineCore_0 pid=93769)\u001b[0;0m INFO 09-09 22:56:25 [kv_cache_utils.py:853] Maximum concurrency for 1,024 tokens per request: 40.91x\n",
      "\u001b[1;36m(EngineCore_0 pid=93769)\u001b[0;0m INFO 09-09 22:56:25 [kv_cache_utils.py:849] GPU KV cache size: 42,544 tokens\n",
      "\u001b[1;36m(EngineCore_0 pid=93769)\u001b[0;0m INFO 09-09 22:56:25 [kv_cache_utils.py:853] Maximum concurrency for 1,024 tokens per request: 40.91x\n",
      "\u001b[1;36m(EngineCore_0 pid=93769)\u001b[0;0m INFO 09-09 22:56:25 [core.py:214] init engine (profile, create kv cache, warmup model) took 3.52 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=93769)\u001b[0;0m INFO 09-09 22:56:26 [__init__.py:3565] Cudagraph is disabled under eager mode\n",
      "INFO 09-09 22:56:27 [llm.py:298] Supported_tasks: ['generate']\n",
      "\u001b[1;36m(EngineCore_0 pid=93769)\u001b[0;0m \u001b[1;36m(EngineCore_0 pid=93769)\u001b[0;0m \u001b[1;36m(VllmWorker TP1 pid=93793)\u001b[0;0m \u001b[1;36m(VllmWorker TP0 pid=93791)\u001b[0;0m WARNING 09-09 22:56:27 [cudagraph_dispatcher.py:101] cudagraph dispatching keys are not initialized. No cudagraph will be used.\n",
      "WARNING 09-09 22:56:27 [cudagraph_dispatcher.py:101] cudagraph dispatching keys are not initialized. No cudagraph will be used.\n"
     ]
    }
   ],
   "source": [
    "# The input text to be processed\n",
    "input_text = \"Lady Juliet gazed longingly at the stars, her heart aching for Romeo\"\n",
    "\n",
    "config = lx.factory.ModelConfig(\n",
    "    model_id=\"vllm:microsoft/Phi-3-mini-4k-instruct\",\n",
    "    \n",
    "    # model_id=\"vllm:Qwen/Qwen3-4B-Instruct-2507\",\n",
    "    provider=\"VLLMLanguageModel\",\n",
    "    provider_kwargs=dict(\n",
    "        gpu_memory_utilization=0.5,\n",
    "        max_model_len=1024,\n",
    "        temperature=0.8,\n",
    "        max_tokens=1024,\n",
    "        # 其他vLLM参数\n",
    "        tensor_parallel_size = 2,\n",
    "        enforce_eager=True,\n",
    "        disable_custom_all_reduce=True,\n",
    "    ),\n",
    ")\n",
    "\n",
    "model = lx.factory.create_model(config)\n",
    "\n",
    "# 添加调试模式来查看模型输出\n",
    "result = lx.extract(\n",
    "    model=model,\n",
    "    text_or_documents=input_text,\n",
    "    prompt_description=prompt,\n",
    "    examples=examples,\n",
    "    fence_output=False,  # 启用fence输出，帮助模型生成更好的JSON\n",
    "    use_schema_constraints=False,\n",
    "    debug=False,  # 启用调试模式\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89008a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".lx-highlight { position: relative; border-radius:3px; padding:1px 2px;}\n",
       ".lx-highlight .lx-tooltip {\n",
       "  visibility: hidden;\n",
       "  opacity: 0;\n",
       "  transition: opacity 0.2s ease-in-out;\n",
       "  background: #333;\n",
       "  color: #fff;\n",
       "  text-align: left;\n",
       "  border-radius: 4px;\n",
       "  padding: 6px 8px;\n",
       "  position: absolute;\n",
       "  z-index: 1000;\n",
       "  bottom: 125%;\n",
       "  left: 50%;\n",
       "  transform: translateX(-50%);\n",
       "  font-size: 12px;\n",
       "  max-width: 240px;\n",
       "  white-space: normal;\n",
       "  box-shadow: 0 2px 6px rgba(0,0,0,0.3);\n",
       "}\n",
       ".lx-highlight:hover .lx-tooltip { visibility: visible; opacity:1; }\n",
       ".lx-animated-wrapper { max-width: 100%; font-family: Arial, sans-serif; }\n",
       ".lx-controls {\n",
       "  background: #fafafa; border: 1px solid #90caf9; border-radius: 8px;\n",
       "  padding: 12px; margin-bottom: 16px;\n",
       "}\n",
       ".lx-button-row {\n",
       "  display: flex; justify-content: center; gap: 8px; margin-bottom: 12px;\n",
       "}\n",
       ".lx-control-btn {\n",
       "  background: #4285f4; color: white; border: none; border-radius: 4px;\n",
       "  padding: 8px 16px; cursor: pointer; font-size: 13px; font-weight: 500;\n",
       "  transition: background-color 0.2s;\n",
       "}\n",
       ".lx-control-btn:hover { background: #3367d6; }\n",
       ".lx-progress-container {\n",
       "  margin-bottom: 8px;\n",
       "}\n",
       ".lx-progress-slider {\n",
       "  width: 100%; margin: 0; appearance: none; height: 6px;\n",
       "  background: #ddd; border-radius: 3px; outline: none;\n",
       "}\n",
       ".lx-progress-slider::-webkit-slider-thumb {\n",
       "  appearance: none; width: 18px; height: 18px; background: #4285f4;\n",
       "  border-radius: 50%; cursor: pointer;\n",
       "}\n",
       ".lx-progress-slider::-moz-range-thumb {\n",
       "  width: 18px; height: 18px; background: #4285f4; border-radius: 50%;\n",
       "  cursor: pointer; border: none;\n",
       "}\n",
       ".lx-status-text {\n",
       "  text-align: center; font-size: 12px; color: #666; margin-top: 4px;\n",
       "}\n",
       ".lx-text-window {\n",
       "  font-family: monospace; white-space: pre-wrap; border: 1px solid #90caf9;\n",
       "  padding: 12px; max-height: 260px; overflow-y: auto; margin-bottom: 12px;\n",
       "  line-height: 1.6;\n",
       "}\n",
       ".lx-attributes-panel {\n",
       "  background: #fafafa; border: 1px solid #90caf9; border-radius: 6px;\n",
       "  padding: 8px 10px; margin-top: 8px; font-size: 13px;\n",
       "}\n",
       ".lx-current-highlight {\n",
       "  border-bottom: 4px solid #ff4444;\n",
       "  font-weight: bold;\n",
       "  animation: lx-pulse 1s ease-in-out;\n",
       "}\n",
       "@keyframes lx-pulse {\n",
       "  0% { text-decoration-color: #ff4444; }\n",
       "  50% { text-decoration-color: #ff0000; }\n",
       "  100% { text-decoration-color: #ff4444; }\n",
       "}\n",
       ".lx-legend {\n",
       "  font-size: 12px; margin-bottom: 8px;\n",
       "  padding-bottom: 8px; border-bottom: 1px solid #e0e0e0;\n",
       "}\n",
       ".lx-label {\n",
       "  display: inline-block;\n",
       "  padding: 2px 4px;\n",
       "  border-radius: 3px;\n",
       "  margin-right: 4px;\n",
       "  color: #000;\n",
       "}\n",
       ".lx-attr-key {\n",
       "  font-weight: 600;\n",
       "  color: #1565c0;\n",
       "  letter-spacing: 0.3px;\n",
       "}\n",
       ".lx-attr-value {\n",
       "  font-weight: 400;\n",
       "  opacity: 0.85;\n",
       "  letter-spacing: 0.2px;\n",
       "}\n",
       "\n",
       "/* Add optimizations with larger fonts and better readability for GIFs */\n",
       ".lx-gif-optimized .lx-text-window { font-size: 16px; line-height: 1.8; }\n",
       ".lx-gif-optimized .lx-attributes-panel { font-size: 15px; }\n",
       ".lx-gif-optimized .lx-current-highlight { text-decoration-thickness: 4px; }\n",
       "</style>\n",
       "<div class=\"lx-animated-wrapper lx-gif-optimized\">\n",
       "  <div class=\"lx-attributes-panel\">\n",
       "    <div class=\"lx-legend\">Highlights Legend: <span class=\"lx-label\" style=\"background-color:#D2E3FC;\">character</span> <span class=\"lx-label\" style=\"background-color:#C8E6C9;\">emotion</span> <span class=\"lx-label\" style=\"background-color:#FEF0C3;\">relationship</span></div>\n",
       "    <div id=\"attributesContainer\"></div>\n",
       "  </div>\n",
       "  <div class=\"lx-text-window\" id=\"textWindow\">\n",
       "    <span class=\"lx-highlight lx-current-highlight\" data-idx=\"0\" style=\"background-color:#D2E3FC;\">Lady Juliet</span> gazed <span class=\"lx-highlight\" data-idx=\"1\" style=\"background-color:#C8E6C9;\">longingly</span> at the stars, her <span class=\"lx-highlight\" data-idx=\"2\" style=\"background-color:#C8E6C9;\">heart aching</span> for <span class=\"lx-highlight\" data-idx=\"3\" style=\"background-color:#FEF0C3;\">Romeo</span>\n",
       "  </div>\n",
       "  <div class=\"lx-controls\">\n",
       "    <div class=\"lx-button-row\">\n",
       "      <button class=\"lx-control-btn\" onclick=\"playPause()\">▶️ Play</button>\n",
       "      <button class=\"lx-control-btn\" onclick=\"prevExtraction()\">⏮ Previous</button>\n",
       "      <button class=\"lx-control-btn\" onclick=\"nextExtraction()\">⏭ Next</button>\n",
       "    </div>\n",
       "    <div class=\"lx-progress-container\">\n",
       "      <input type=\"range\" id=\"progressSlider\" class=\"lx-progress-slider\"\n",
       "             min=\"0\" max=\"3\" value=\"0\"\n",
       "             onchange=\"jumpToExtraction(this.value)\">\n",
       "    </div>\n",
       "    <div class=\"lx-status-text\">\n",
       "      Entity <span id=\"entityInfo\">1/4</span> |\n",
       "      Pos <span id=\"posInfo\">[0-11]</span>\n",
       "    </div>\n",
       "  </div>\n",
       "</div>\n",
       "\n",
       "<script>\n",
       "  (function() {\n",
       "    const extractions = [{\"index\": 0, \"class\": \"character\", \"text\": \"Lady Juliet\", \"color\": \"#D2E3FC\", \"startPos\": 0, \"endPos\": 11, \"beforeText\": \"\", \"extractionText\": \"Lady Juliet\", \"afterText\": \" gazed longingly at the stars, her heart aching for Romeo\", \"attributesHtml\": \"<div><strong>class:</strong> character</div><div><strong>attributes:</strong> {<span class=\\\"lx-attr-key\\\">emotional_state</span>: <span class=\\\"lx-attr-value\\\">longing</span>}</div>\"}, {\"index\": 1, \"class\": \"emotion\", \"text\": \"longingly\", \"color\": \"#C8E6C9\", \"startPos\": 18, \"endPos\": 27, \"beforeText\": \"Lady Juliet gazed \", \"extractionText\": \"longingly\", \"afterText\": \" at the stars, her heart aching for Romeo\", \"attributesHtml\": \"<div><strong>class:</strong> emotion</div><div><strong>attributes:</strong> {<span class=\\\"lx-attr-key\\\">feeling</span>: <span class=\\\"lx-attr-value\\\">yearning</span>}</div>\"}, {\"index\": 2, \"class\": \"emotion\", \"text\": \"heart aching\", \"color\": \"#C8E6C9\", \"startPos\": 46, \"endPos\": 58, \"beforeText\": \"Lady Juliet gazed longingly at the stars, her \", \"extractionText\": \"heart aching\", \"afterText\": \" for Romeo\", \"attributesHtml\": \"<div><strong>class:</strong> emotion</div><div><strong>attributes:</strong> {<span class=\\\"lx-attr-key\\\">feeling</span>: <span class=\\\"lx-attr-value\\\">pain</span>}</div>\"}, {\"index\": 3, \"class\": \"relationship\", \"text\": \"Romeo\", \"color\": \"#FEF0C3\", \"startPos\": 63, \"endPos\": 68, \"beforeText\": \"Lady Juliet gazed longingly at the stars, her heart aching for \", \"extractionText\": \"Romeo\", \"afterText\": \"\", \"attributesHtml\": \"<div><strong>class:</strong> relationship</div><div><strong>attributes:</strong> {<span class=\\\"lx-attr-key\\\">type</span>: <span class=\\\"lx-attr-value\\\">object_of_affection</span>}</div>\"}];\n",
       "    let currentIndex = 0;\n",
       "    let isPlaying = false;\n",
       "    let animationInterval = null;\n",
       "    let animationSpeed = 1.0;\n",
       "\n",
       "    function updateDisplay() {\n",
       "      const extraction = extractions[currentIndex];\n",
       "      if (!extraction) return;\n",
       "\n",
       "      document.getElementById('attributesContainer').innerHTML = extraction.attributesHtml;\n",
       "      document.getElementById('entityInfo').textContent = (currentIndex + 1) + '/' + extractions.length;\n",
       "      document.getElementById('posInfo').textContent = '[' + extraction.startPos + '-' + extraction.endPos + ']';\n",
       "      document.getElementById('progressSlider').value = currentIndex;\n",
       "\n",
       "      const playBtn = document.querySelector('.lx-control-btn');\n",
       "      if (playBtn) playBtn.textContent = isPlaying ? '⏸ Pause' : '▶️ Play';\n",
       "\n",
       "      const prevHighlight = document.querySelector('.lx-text-window .lx-current-highlight');\n",
       "      if (prevHighlight) prevHighlight.classList.remove('lx-current-highlight');\n",
       "      const currentSpan = document.querySelector('.lx-text-window span[data-idx=\"' + currentIndex + '\"]');\n",
       "      if (currentSpan) {\n",
       "        currentSpan.classList.add('lx-current-highlight');\n",
       "        currentSpan.scrollIntoView({block: 'center', behavior: 'smooth'});\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function nextExtraction() {\n",
       "      currentIndex = (currentIndex + 1) % extractions.length;\n",
       "      updateDisplay();\n",
       "    }\n",
       "\n",
       "    function prevExtraction() {\n",
       "      currentIndex = (currentIndex - 1 + extractions.length) % extractions.length;\n",
       "      updateDisplay();\n",
       "    }\n",
       "\n",
       "    function jumpToExtraction(index) {\n",
       "      currentIndex = parseInt(index);\n",
       "      updateDisplay();\n",
       "    }\n",
       "\n",
       "    function playPause() {\n",
       "      if (isPlaying) {\n",
       "        clearInterval(animationInterval);\n",
       "        isPlaying = false;\n",
       "      } else {\n",
       "        animationInterval = setInterval(nextExtraction, animationSpeed * 1000);\n",
       "        isPlaying = true;\n",
       "      }\n",
       "      updateDisplay();\n",
       "    }\n",
       "\n",
       "    window.playPause = playPause;\n",
       "    window.nextExtraction = nextExtraction;\n",
       "    window.prevExtraction = prevExtraction;\n",
       "    window.jumpToExtraction = jumpToExtraction;\n",
       "\n",
       "    updateDisplay();\n",
       "  })();\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# result 已经是一个 AnnotatedDocument 对象，不需要索引访问\n",
    "result = cast(AnnotatedDocument, result)\n",
    "\n",
    "html_content = lx.visualize(result)\n",
    "html_content = cast(HTML, html_content)\n",
    "\n",
    "display(html_content)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
