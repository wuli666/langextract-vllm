{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0116dda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import textwrap\n",
    "from typing import cast\n",
    "\n",
    "# 设置Hugging Face镜像（中国用户）\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "\n",
    "import langextract as lx\n",
    "from IPython.display import HTML\n",
    "from langextract.data import AnnotatedDocument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bdb140b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the prompt and extraction rules\n",
    "prompt = textwrap.dedent(\"\"\"\\\n",
    "    Extract characters, emotions, and relationships in order of appearance.\n",
    "    Use exact text for extractions. Do not paraphrase or overlap entities.\n",
    "    Provide meaningful attributes for each entity to add context.\"\"\")\n",
    "\n",
    "# 2. Provide a high-quality example to guide the model\n",
    "examples = [\n",
    "    lx.data.ExampleData(\n",
    "        text=\"ROMEO. But soft! What light through yonder window breaks? It is the east, and Juliet is the sun.\",\n",
    "        extractions=[\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"character\",\n",
    "                extraction_text=\"ROMEO\",\n",
    "                attributes={\"emotional_state\": \"wonder\"},\n",
    "            ),\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"emotion\",\n",
    "                extraction_text=\"But soft!\",\n",
    "                attributes={\"feeling\": \"gentle awe\"},\n",
    "            ),\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"relationship\",\n",
    "                extraction_text=\"Juliet is the sun\",\n",
    "                attributes={\"type\": \"metaphor\"},\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2e790dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-09 16:26:51 [utils.py:326] non-default args: {'model': 'Qwen/Qwen3-4B-Instruct-2507', 'max_model_len': 1024, 'gpu_memory_utilization': 0.5, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True}\n",
      "INFO 09-09 16:26:53 [__init__.py:711] Resolved architecture: Qwen3ForCausalLM\n",
      "INFO 09-09 16:26:53 [__init__.py:1750] Using max model len 1024\n",
      "INFO 09-09 16:26:55 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 09-09 16:26:55 [__init__.py:3565] Cudagraph is disabled under eager mode\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m INFO 09-09 16:26:57 [core.py:636] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m INFO 09-09 16:26:57 [core.py:74] Initializing a V1 LLM engine (v0.10.1.1) with config: model='Qwen/Qwen3-4B-Instruct-2507', speculative_config=None, tokenizer='Qwen/Qwen3-4B-Instruct-2507', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-4B-Instruct-2507, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":0,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":0,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m INFO 09-09 16:26:58 [parallel_state.py:1134] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m WARNING 09-09 16:26:58 [topk_topp_sampler.py:61] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m INFO 09-09 16:26:58 [gpu_model_runner.py:1953] Starting to load model Qwen/Qwen3-4B-Instruct-2507...\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m INFO 09-09 16:26:58 [gpu_model_runner.py:1985] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m INFO 09-09 16:26:58 [cuda.py:328] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m INFO 09-09 16:26:59 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:00<00:01,  1.16it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:01<00:00,  1.96it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:01<00:00,  1.83it/s]\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m INFO 09-09 16:27:01 [default_loader.py:262] Loading weights took 1.70 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m INFO 09-09 16:27:01 [gpu_model_runner.py:2007] Model loading took 7.6065 GiB and 3.003432 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700] EngineCore failed to start.\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700] Traceback (most recent call last):\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]   File \"/home/djj/anaconda3/envs/py311/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 2460, in _dummy_sampler_run\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]     sampler_output = self.sampler(logits=logits,\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]   File \"/home/djj/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]   File \"/home/djj/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]   File \"/home/djj/anaconda3/envs/py311/lib/python3.11/site-packages/vllm/v1/sample/sampler.py\", line 68, in forward\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]     sampled = self.sample(logits, sampling_metadata)\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]   File \"/home/djj/anaconda3/envs/py311/lib/python3.11/site-packages/vllm/v1/sample/sampler.py\", line 135, in sample\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]     random_sampled = self.topk_topp_sampler(\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]   File \"/home/djj/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]   File \"/home/djj/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]   File \"/home/djj/anaconda3/envs/py311/lib/python3.11/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py\", line 83, in forward_native\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]     logits = apply_top_k_top_p(logits, k, p)\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]   File \"/home/djj/anaconda3/envs/py311/lib/python3.11/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py\", line 189, in apply_top_k_top_p\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 298.00 MiB. GPU 0 has a total capacity of 11.61 GiB of which 175.88 MiB is free. Including non-PyTorch memory, this process has 8.87 GiB memory in use. Of the allocated memory 8.35 GiB is allocated by PyTorch, and 229.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700] \n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700] The above exception was the direct cause of the following exception:\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700] \n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700] Traceback (most recent call last):\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]   File \"/home/djj/anaconda3/envs/py311/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 691, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]   File \"/home/djj/anaconda3/envs/py311/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 492, in __init__\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]     super().__init__(vllm_config, executor_class, log_stats,\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]   File \"/home/djj/anaconda3/envs/py311/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 89, in __init__\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]     self._initialize_kv_caches(vllm_config)\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]   File \"/home/djj/anaconda3/envs/py311/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 179, in _initialize_kv_caches\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]     self.model_executor.determine_available_memory())\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]   File \"/home/djj/anaconda3/envs/py311/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 76, in determine_available_memory\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]     output = self.collective_rpc(\"determine_available_memory\")\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]   File \"/home/djj/anaconda3/envs/py311/lib/python3.11/site-packages/vllm/executor/uniproc_executor.py\", line 58, in collective_rpc\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]     answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]   File \"/home/djj/anaconda3/envs/py311/lib/python3.11/site-packages/vllm/utils/__init__.py\", line 3007, in run_method\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]   File \"/home/djj/anaconda3/envs/py311/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]   File \"/home/djj/anaconda3/envs/py311/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 244, in determine_available_memory\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]     self.model_runner.profile_run()\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]   File \"/home/djj/anaconda3/envs/py311/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 2627, in profile_run\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]     output = self._dummy_sampler_run(last_hidden_states)\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]   File \"/home/djj/anaconda3/envs/py311/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]   File \"/home/djj/anaconda3/envs/py311/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 2464, in _dummy_sampler_run\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700]     raise RuntimeError(\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m ERROR 09-09 16:27:03 [core.py:700] RuntimeError: CUDA out of memory occurred when warming up sampler with 256 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m Process EngineCore_0:\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m Traceback (most recent call last):\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m   File \"/home/djj/anaconda3/envs/py311/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 2460, in _dummy_sampler_run\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m     sampler_output = self.sampler(logits=logits,\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m   File \"/home/djj/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m   File \"/home/djj/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m   File \"/home/djj/anaconda3/envs/py311/lib/python3.11/site-packages/vllm/v1/sample/sampler.py\", line 68, in forward\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m     sampled = self.sample(logits, sampling_metadata)\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m   File \"/home/djj/anaconda3/envs/py311/lib/python3.11/site-packages/vllm/v1/sample/sampler.py\", line 135, in sample\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m     random_sampled = self.topk_topp_sampler(\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m   File \"/home/djj/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m   File \"/home/djj/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m   File \"/home/djj/anaconda3/envs/py311/lib/python3.11/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py\", line 83, in forward_native\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m     logits = apply_top_k_top_p(logits, k, p)\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m   File \"/home/djj/anaconda3/envs/py311/lib/python3.11/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py\", line 189, in apply_top_k_top_p\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 298.00 MiB. GPU 0 has a total capacity of 11.61 GiB of which 175.88 MiB is free. Including non-PyTorch memory, this process has 8.87 GiB memory in use. Of the allocated memory 8.35 GiB is allocated by PyTorch, and 229.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m \n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m The above exception was the direct cause of the following exception:\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m \n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m Traceback (most recent call last):\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m   File \"/home/djj/anaconda3/envs/py311/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m     self.run()\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m   File \"/home/djj/anaconda3/envs/py311/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m   File \"/home/djj/anaconda3/envs/py311/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 704, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m     raise e\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m   File \"/home/djj/anaconda3/envs/py311/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 691, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m   File \"/home/djj/anaconda3/envs/py311/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 492, in __init__\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m     super().__init__(vllm_config, executor_class, log_stats,\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m   File \"/home/djj/anaconda3/envs/py311/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 89, in __init__\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m     self._initialize_kv_caches(vllm_config)\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m   File \"/home/djj/anaconda3/envs/py311/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 179, in _initialize_kv_caches\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m     self.model_executor.determine_available_memory())\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m   File \"/home/djj/anaconda3/envs/py311/lib/python3.11/site-packages/vllm/v1/executor/abstract.py\", line 76, in determine_available_memory\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m     output = self.collective_rpc(\"determine_available_memory\")\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m   File \"/home/djj/anaconda3/envs/py311/lib/python3.11/site-packages/vllm/executor/uniproc_executor.py\", line 58, in collective_rpc\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m     answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m   File \"/home/djj/anaconda3/envs/py311/lib/python3.11/site-packages/vllm/utils/__init__.py\", line 3007, in run_method\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m   File \"/home/djj/anaconda3/envs/py311/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m   File \"/home/djj/anaconda3/envs/py311/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 244, in determine_available_memory\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m     self.model_runner.profile_run()\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m   File \"/home/djj/anaconda3/envs/py311/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 2627, in profile_run\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m     output = self._dummy_sampler_run(last_hidden_states)\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m   File \"/home/djj/anaconda3/envs/py311/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m   File \"/home/djj/anaconda3/envs/py311/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 2464, in _dummy_sampler_run\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m     raise RuntimeError(\n",
      "\u001b[1;36m(EngineCore_0 pid=70423)\u001b[0;0m RuntimeError: CUDA out of memory occurred when warming up sampler with 256 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Engine core initialization failed. See root cause above. Failed core proc(s): {}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m      2\u001b[39m input_text = \u001b[33m\"\u001b[39m\u001b[33mLady Juliet gazed longingly at the stars, her heart aching for Romeo\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m config = lx.factory.ModelConfig(\n\u001b[32m      5\u001b[39m     model_id=\u001b[33m\"\u001b[39m\u001b[33mvllm:Qwen/Qwen3-4B-Instruct-2507\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     provider=\u001b[33m\"\u001b[39m\u001b[33mVLLMLanguageModel\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m     ),\n\u001b[32m     16\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m model = \u001b[43mlx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfactory\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# 添加调试模式来查看模型输出\u001b[39;00m\n\u001b[32m     21\u001b[39m result = lx.extract(\n\u001b[32m     22\u001b[39m     model=model,\n\u001b[32m     23\u001b[39m     text_or_documents=input_text,\n\u001b[32m   (...)\u001b[39m\u001b[32m     28\u001b[39m     debug=\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# 启用调试模式\u001b[39;00m\n\u001b[32m     29\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/py311/lib/python3.11/site-packages/langextract/factory.py:155\u001b[39m, in \u001b[36mcreate_model\u001b[39m\u001b[34m(config, examples, use_schema_constraints, fence_output, return_fence_output)\u001b[39m\n\u001b[32m    152\u001b[39m   kwargs[\u001b[33m\"\u001b[39m\u001b[33mmodel_id\u001b[39m\u001b[33m\"\u001b[39m] = model_id\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m   model = \u001b[43mprovider_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m return_fence_output:\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model, model.requires_fence_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/langextract/langextract-vllm/langextract_vllm/provider.py:75\u001b[39m, in \u001b[36mVLLMLanguageModel.__init__\u001b[39m\u001b[34m(self, model_id, max_workers, temperature, top_p, max_tokens, gpu_memory_utilization, max_model_len, **kwargs)\u001b[39m\n\u001b[32m     72\u001b[39m engine_kwargs.setdefault(\u001b[33m'\u001b[39m\u001b[33menforce_eager\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     73\u001b[39m engine_kwargs.setdefault(\u001b[33m'\u001b[39m\u001b[33mdisable_custom_all_reduce\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m \u001b[38;5;28mself\u001b[39m._vllm_engine = \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mactual_model_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_model_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_model_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mengine_kwargs\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;66;03m# Configure sampling parameters\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[38;5;28mself\u001b[39m._sampling_params = SamplingParams(\n\u001b[32m     84\u001b[39m     temperature=temperature,\n\u001b[32m     85\u001b[39m     top_p=top_p,\n\u001b[32m     86\u001b[39m     max_tokens=max_tokens,\n\u001b[32m     87\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/py311/lib/python3.11/site-packages/vllm/entrypoints/llm.py:285\u001b[39m, in \u001b[36mLLM.__init__\u001b[39m\u001b[34m(self, model, runner, convert, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_token, hf_overrides, mm_processor_kwargs, override_pooler_config, compilation_config, logits_processors, **kwargs)\u001b[39m\n\u001b[32m    282\u001b[39m log_non_default_args(engine_args)\n\u001b[32m    284\u001b[39m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m \u001b[38;5;28mself\u001b[39m.llm_engine = \u001b[43mLLMEngine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUsageContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_class = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm_engine)\n\u001b[32m    289\u001b[39m \u001b[38;5;28mself\u001b[39m.request_counter = Counter()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/py311/lib/python3.11/site-packages/vllm/engine/llm_engine.py:490\u001b[39m, in \u001b[36mLLMEngine.from_engine_args\u001b[39m\u001b[34m(cls, engine_args, usage_context, stat_loggers)\u001b[39m\n\u001b[32m    487\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv1\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllm_engine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLMEngine \u001b[38;5;28;01mas\u001b[39;00m V1LLMEngine\n\u001b[32m    488\u001b[39m     engine_cls = V1LLMEngine\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine_cls\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_vllm_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/py311/lib/python3.11/site-packages/vllm/v1/engine/llm_engine.py:127\u001b[39m, in \u001b[36mLLMEngine.from_vllm_config\u001b[39m\u001b[34m(cls, vllm_config, usage_context, stat_loggers, disable_log_stats)\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_vllm_config\u001b[39m(\n\u001b[32m    121\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    125\u001b[39m     disable_log_stats: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    126\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mLLMEngine\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m               \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mExecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m               \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m               \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m               \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m               \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43menvs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mVLLM_ENABLE_V1_MULTIPROCESSING\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/py311/lib/python3.11/site-packages/vllm/v1/engine/llm_engine.py:104\u001b[39m, in \u001b[36mLLMEngine.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, mm_registry, use_cached_outputs, multiprocess_mode)\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28mself\u001b[39m.output_processor = OutputProcessor(\u001b[38;5;28mself\u001b[39m.tokenizer,\n\u001b[32m    101\u001b[39m                                         log_stats=\u001b[38;5;28mself\u001b[39m.log_stats)\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# EngineCore (gets EngineCoreRequests and gives EngineCoreOutputs)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_core = \u001b[43mEngineCoreClient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m    \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m multiprocess_mode:\n\u001b[32m    113\u001b[39m     \u001b[38;5;66;03m# for v0 compatibility\u001b[39;00m\n\u001b[32m    114\u001b[39m     \u001b[38;5;28mself\u001b[39m.model_executor = \u001b[38;5;28mself\u001b[39m.engine_core.engine_core.model_executor  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/py311/lib/python3.11/site-packages/vllm/v1/engine/core_client.py:80\u001b[39m, in \u001b[36mEngineCoreClient.make_client\u001b[39m\u001b[34m(multiprocess_mode, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m EngineCoreClient.make_async_mp_client(\n\u001b[32m     77\u001b[39m         vllm_config, executor_class, log_stats)\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m multiprocess_mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio_mode:\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSyncMPClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m InprocClient(vllm_config, executor_class, log_stats)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/py311/lib/python3.11/site-packages/vllm/v1/engine/core_client.py:600\u001b[39m, in \u001b[36mSyncMPClient.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m    598\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, vllm_config: VllmConfig, executor_class: \u001b[38;5;28mtype\u001b[39m[Executor],\n\u001b[32m    599\u001b[39m              log_stats: \u001b[38;5;28mbool\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m600\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[43m        \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    602\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    604\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m     \u001b[38;5;28mself\u001b[39m.is_dp = \u001b[38;5;28mself\u001b[39m.vllm_config.parallel_config.data_parallel_size > \u001b[32m1\u001b[39m\n\u001b[32m    608\u001b[39m     \u001b[38;5;28mself\u001b[39m.outputs_queue = queue.Queue[Union[EngineCoreOutputs, \u001b[38;5;167;01mException\u001b[39;00m]]()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/py311/lib/python3.11/site-packages/vllm/v1/engine/core_client.py:446\u001b[39m, in \u001b[36mMPClient.__init__\u001b[39m\u001b[34m(self, asyncio_mode, vllm_config, executor_class, log_stats, client_addresses)\u001b[39m\n\u001b[32m    442\u001b[39m     \u001b[38;5;28mself\u001b[39m.stats_update_address = client_addresses.get(\n\u001b[32m    443\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstats_update_address\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    444\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    445\u001b[39m     \u001b[38;5;66;03m# Engines are managed by this client.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m446\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlaunch_core_engines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m                                            \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m                                            \u001b[49m\u001b[43maddresses\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresources\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoordinator\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresources\u001b[49m\u001b[43m.\u001b[49m\u001b[43mengine_manager\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine_manager\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/py311/lib/python3.11/contextlib.py:144\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m         \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.gen)\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    146\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/py311/lib/python3.11/site-packages/vllm/v1/engine/utils.py:706\u001b[39m, in \u001b[36mlaunch_core_engines\u001b[39m\u001b[34m(vllm_config, executor_class, log_stats, num_api_servers)\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m local_engine_manager, coordinator, addresses\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Now wait for engines to start.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m706\u001b[39m \u001b[43mwait_for_engine_startup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhandshake_socket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[43m    \u001b[49m\u001b[43maddresses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengines_to_handshake\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_engine_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/py311/lib/python3.11/site-packages/vllm/v1/engine/utils.py:759\u001b[39m, in \u001b[36mwait_for_engine_startup\u001b[39m\u001b[34m(handshake_socket, addresses, core_engines, parallel_config, cache_config, proc_manager, coord_process)\u001b[39m\n\u001b[32m    757\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m coord_process \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m coord_process.exitcode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    758\u001b[39m         finished[coord_process.name] = coord_process.exitcode\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mEngine core initialization failed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    760\u001b[39m                        \u001b[33m\"\u001b[39m\u001b[33mSee root cause above. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    761\u001b[39m                        \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed core proc(s): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinished\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    763\u001b[39m \u001b[38;5;66;03m# Receive HELLO and READY messages from the input socket.\u001b[39;00m\n\u001b[32m    764\u001b[39m eng_identity, ready_msg_bytes = handshake_socket.recv_multipart()\n",
      "\u001b[31mRuntimeError\u001b[39m: Engine core initialization failed. See root cause above. Failed core proc(s): {}"
     ]
    }
   ],
   "source": [
    "# The input text to be processed\n",
    "input_text = \"Lady Juliet gazed longingly at the stars, her heart aching for Romeo\"\n",
    "\n",
    "config = lx.factory.ModelConfig(\n",
    "    model_id=\"vllm:Qwen/Qwen3-4B-Instruct-2507\",\n",
    "    provider=\"VLLMLanguageModel\",\n",
    "    provider_kwargs=dict(\n",
    "        gpu_memory_utilization=0.5,\n",
    "        max_model_len=1024,\n",
    "        temperature=0.8,\n",
    "        max_tokens=512,\n",
    "        # 其他vLLM参数\n",
    "        enforce_eager=True,\n",
    "        disable_custom_all_reduce=True,\n",
    "    ),\n",
    ")\n",
    "\n",
    "model = lx.factory.create_model(config)\n",
    "\n",
    "# 添加调试模式来查看模型输出\n",
    "result = lx.extract(\n",
    "    model=model,\n",
    "    text_or_documents=input_text,\n",
    "    prompt_description=prompt,\n",
    "    examples=examples,\n",
    "    fence_output=True,  # 启用fence输出，帮助模型生成更好的JSON\n",
    "    use_schema_constraints=False,\n",
    "    debug=True,  # 启用调试模式\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab557f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型测试结果:\n",
      "Prompt 1:  I am 23 years old, and I have a passion for something. I am in the middle of a high school year, and I am in the first year of high school. I have a strong sense of discipline and ability to work. I want to go to university and study in the same field as my passion. I have a dream of becoming a doctor. However, I have a lot of obstacles and challenges, and I am struggling to find my passion and my dreams. What should I do?\n",
      "I am 23, in high school, first year, I have a strong sense of discipline and ability to work. I want to go to university and study in the same field as my passion. I have a dream of becoming a doctor. However, I have a lot of obstacles and challenges, and I am struggling to find my passion and my dreams. What should I do?\n",
      "\n",
      "I have a dream of becoming a doctor, but I am struggling to find my passion and my dreams. What should I do?\n",
      "\n",
      "I have a dream of becoming a doctor, but I am struggling to find my passion and my dreams. What should I do?\n",
      "\n",
      "Hello, my name is Alex, I am 23 years old, and I am in the first year of high school. I am in a high school where I have a strong sense of discipline and ability to work. I have a dream of becoming a doctor. However, I have a lot of obstacles and challenges, and I am struggling to find my passion and my dreams. What should I do?\n",
      "\n",
      "I am 23, first year of high school, strong sense of discipline and ability to work. I have a dream of becoming a doctor. However, I am struggling to find my passion and dreams. What should I do?\n",
      "\n",
      "Hi, I am 23, in the first year of high school, I have a strong sense of discipline and ability to work. I have a dream of becoming a doctor. However, I am struggling to find my passion and dreams. What should I do?\n",
      "\n",
      "I am 23, first year of high school, strong sense of discipline and ability to work. I have a dream of becoming a doctor. However, I am struggling to find my passion and dreams. What should I do?\n",
      "\n",
      "I am 23, in the first year of high school. I have a strong sense of discipline and ability to work. I have a dream of becoming a doctor. However, I am struggling to find my passion and dreams. What should\n"
     ]
    }
   ],
   "source": [
    "# 先测试模型是否能正常生成文本\n",
    "test_prompts = [\"Hello, how are you?\"]\n",
    "test_results = list(model.infer(test_prompts))\n",
    "print(\"模型测试结果:\")\n",
    "for i, result in enumerate(test_results):\n",
    "    print(f\"Prompt {i+1}: {result[0].output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcb5e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c75c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 尝试使用更简单的方法 - 直接测试模型输出\n",
    "print(\"=== 测试模型是否能生成JSON ===\")\n",
    "\n",
    "# 创建一个简单的JSON生成测试\n",
    "test_prompt = \"\"\"Extract characters from: \"Romeo loves Juliet\"\n",
    "Return ONLY this JSON:\n",
    "{\"extractions\": [{\"extraction_class\": \"character\", \"extraction_text\": \"Romeo\", \"attributes\": {}}, {\"extraction_class\": \"character\", \"extraction_text\": \"Juliet\", \"attributes\": {}}]}\"\"\"\n",
    "\n",
    "test_results = list(model.infer([test_prompt]))\n",
    "print(\"模型输出:\")\n",
    "print(test_results[0][0].output)\n",
    "print(\"\\n\" + \"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4459bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新配置模型，使用更保守的内存设置\n",
    "print(\"=== 重新配置模型 ===\")\n",
    "\n",
    "config = lx.factory.ModelConfig(\n",
    "    model_id=\"vllm:microsoft/DialoGPT-small\",\n",
    "    provider=\"VLLMLanguageModel\",\n",
    "    provider_kwargs=dict(\n",
    "        gpu_memory_utilization=0.2,  # 大幅降低GPU内存使用\n",
    "        max_model_len=256,           # 大幅减少序列长度\n",
    "        max_num_seqs=1,              # 减少并发序列数\n",
    "        temperature=0.0,             # 使用贪婪解码\n",
    "        max_tokens=64,               # 大幅减少输出长度\n",
    "        # 其他vLLM参数\n",
    "        enforce_eager=True,\n",
    "        disable_custom_all_reduce=True,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"创建模型...\")\n",
    "model = lx.factory.create_model(config)\n",
    "print(\"模型创建成功！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee206c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = cast(AnnotatedDocument, result)\n",
    "\n",
    "html_content = lx.visualize(result)\n",
    "html_content = cast(HTML, html_content)\n",
    "\n",
    "display(html_content) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
